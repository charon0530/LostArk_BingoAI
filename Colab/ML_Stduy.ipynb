{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Stduy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNf/EWGaw9GY65epYaAad5T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n"],"metadata":{"id":"SwcUcbwlZ-Me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def AND(x1,x2):\n","  x=np.array([x1,x2])\n","  w = np.array([0.5,0.5])\n","  b= -0.7\n","  tmp = np.sum(w*x) + b\n","  if tmp<=0:\n","    return 0\n","  else:\n","    return 1"],"metadata":{"id":"-LG-9e_0h9Wo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(AND(0,0))\n","print(AND(1,0))\n","print(AND(0,1))\n","print(AND(1,1))\n"],"metadata":{"id":"CwHp9jlriPkG","executionInfo":{"status":"ok","timestamp":1642118009203,"user_tz":-540,"elapsed":9,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"88a5dc5e-46c0-4d65-c9d5-15459f1acbd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","0\n","0\n","1\n"]}]},{"cell_type":"code","source":["def step_function(x):\n","  y=x>0\n","  return y.astype(np.int)"],"metadata":{"id":"ec_Oyw8Ktpl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x=np.arange(-5.0,5.0,0.1)\n","y=step_function(x)\n","plt.plot(x,y)\n","plt.ylim(-0.1,1.1)\n","plt.show()"],"metadata":{"id":"j5yxKWGQuIuq","executionInfo":{"status":"ok","timestamp":1642118009660,"user_tz":-540,"elapsed":463,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"2bd14da9-c4fe-424f-e967-d78dad6ff714"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["한 층에서의 노드들은 각각의 역할이 있다.\n","ex) AND 게이트 OR 게이트"],"metadata":{"id":"sgLKrgjJxmt1"}},{"cell_type":"markdown","source":["활성화 함수로 비선형이 아닌 선형 함수를 사용한다면 단층으로 표현이 가능해진다.\n","\n","즉, AND AND AND 이렇게 있을 때 비선형이라면 첫번째 AND에서 이상한 값(선형적이지 않은 값)을 받고 넘기고... 를 반복해서 특이한 그래프모양을 갖을 수 있겠지만\n","\n","선형이라면 AND 하나로 출력량만 조절해서 표현 할 수 있다.\n","\n","=> 즉, 비선형이면 XOR을 만들 수 있지만 선형이라면 XOR을 만들 수 없다."],"metadata":{"id":"Q-zxHc4h0Xx1"}},{"cell_type":"code","source":["A=np.array([[1,2],[3,4],[5,6]])\n","#B= np.array([[7],[8]])\n","B=np.array([7,8])\n","print(B.shape)\n","print(np.dot(A,B))\n"],"metadata":{"id":"80Xx3eEP1e8L","executionInfo":{"status":"ok","timestamp":1642118009663,"user_tz":-540,"elapsed":32,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e55590f4-7b39-4d9e-fea6-b9bfbc82ca54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2,)\n","[23 53 83]\n"]}]},{"cell_type":"code","source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n"],"metadata":{"id":"FjE9MiFu7y88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([1.0,0.5])\n","W1 = np.array(([[0.1,0.3,0.5],[0.2,0.4,0.6]]))\n","B1 = np.array([0.1,0.2,0.3])\n","print(W1.shape)\n","print(X.shape)\n","print(B1.shape)\n","A1=np.dot(X,W1)+B1\n","print(A1)\n","Z1 = sigmoid(A1)\n","\n","W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n","B2 = np.array([0.1,0.2])\n","\n","A2= np.dot(Z1,W2) + B2\n","Z2 = sigmoid(A2)\n","\n","def identity_function(x):\n","  return x\n","\n","W3 = np.array([[0.1,0.3],[0.2,0.4]])\n","B3 = np.array([0.1,0.2])\n","\n","A3 = np.dot(Z2,W3) + B3\n","Y = identity_function(A3)"],"metadata":{"id":"F8WrXmwa6gd_","executionInfo":{"status":"ok","timestamp":1642118009665,"user_tz":-540,"elapsed":31,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d045ab7-a09e-4370-bb28-da9a246acc00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 3)\n","(2,)\n","(3,)\n","[0.3 0.7 1.1]\n"]}]},{"cell_type":"markdown","source":["89 페이지 까지 봄"],"metadata":{"id":"pciDeif9--1M"}},{"cell_type":"code","source":["def init_network():\n","  network = {}\n","  #2x3\n","  network[\"W1\"] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n","  #3\n","  network[\"b1\"] = np.array([0.1,0.2,0.3])\n","  #3x2\n","  network[\"W2\"] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n","  #2\n","  network[\"b2\"] = np.array([0.1,0.2])\n","  #2x2\n","  network[\"W3\"] = np.array([[0.1,0.3],[0.2,0.4]])\n","  #2\n","  network[\"b3\"] = np.array([0.1,0.2])\n","\n","  return network\n"],"metadata":{"id":"p0vpbWRngIk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def forward(network,x):\n","  W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n","  b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n","\n","  a1 = np.dot(x,W1) + b1 # 3\n","  z1 = sigmoid(a1)\n","\n","  a2 = np.dot(z1,W2) + b2 # 2\n","  z2 = sigmoid(a2)\n","\n","  a3 = np.dot(z2,W3) + b3# 2\n","  y = identity_function(a3)\n","\n","  return y"],"metadata":{"id":"ljP0oly1hhgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["network = init_network()\n","x = np.array([1.0,0.5])\n","y = forward(network, x)\n","print(y)"],"metadata":{"id":"94ls1OsCj4Jq","executionInfo":{"status":"ok","timestamp":1642118009668,"user_tz":-540,"elapsed":32,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5ab9cea8-fce7-4fc9-fe37-f04a9468d847"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.31682708 0.69627909]\n"]}]},{"cell_type":"code","source":["#def softmax(a):\n","#  c = np.max(a)\n","#  exp_a = np.exp(a-c)\n","#  sum_exp_a = np.sum(exp_a)\n","#  y = exp_a / sum_exp_a\n","#\n","#  return y\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))"],"metadata":{"id":"oUXw-tivmAv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["softmax(np.array([0.3,2.9,4.0]))"],"metadata":{"id":"lH-ZIPmMmv6v","executionInfo":{"status":"ok","timestamp":1642118009671,"user_tz":-540,"elapsed":32,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d768670-8c2e-40c5-da61-4aedaade7734"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.01821127, 0.24519181, 0.73659691])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["각 노드(뉴런)들은 자기만의 역할이 있다.\n","\n","ex) MNIST에서 출력층의 0번 노드는 숫자 0을, 1번 노드는 숫자 1을... 판단하는 역할을 한다. \n","\n","또한 은닉층의 낮은 층, 예를 들어 은닉층 1층의 경우 이미지에서 특정 기울기에 반응, 판단하는 역할을 한다."],"metadata":{"id":"tt8ltrlk2VpC"}},{"cell_type":"markdown","source":["기계학습(머신러닝) : \"사람이 특징을 추출\" 후 학습\n","\n","딥러닝 : 컴퓨터가 특징을 추출 후 학습"],"metadata":{"id":"YzL2SHiu_220"}},{"cell_type":"code","source":["arr = np.array([10,11,12,13,14,15,16,17,18,19])\n","arr[[3,7]]"],"metadata":{"id":"DOqiC-1W3Mjw","executionInfo":{"status":"ok","timestamp":1642118009671,"user_tz":-540,"elapsed":30,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c915f59-077d-4cef-8af5-fab35a1293c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([13, 17])"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n","\n","print(arr[[0,1,2],[0,1,2]])\n"],"metadata":{"id":"wOKSvJcynQrI","executionInfo":{"status":"ok","timestamp":1642118009672,"user_tz":-540,"elapsed":29,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47c3552d-0a33-4ae2-f428-0fee04dbb4cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 5 9]\n"]}]},{"cell_type":"markdown","source":["활성화 함수로 계단 함수보다 시그모이드 함수를 사용하는 이유는 가중치 미세조정을 할 때 계단 함수는 조정에 따른 값의 전달이 없어져 버리기 때문이다."],"metadata":{"id":"u5JlOt1OvonI"}},{"cell_type":"code","source":["def function_2(x):\n","  return x[0]**2 + x[1]**2"],"metadata":{"id":"FmX3BHes0IKY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def numerical_gradient(f,x):\n","  h = 1e-4\n","  grad = np.zeros_like(x)\n","\n","  for idx in range(x.size):\n","    temp_val = x[idx]\n","    x[idx] = temp_val + h\n","    fxh1 = f(x)\n","\n","    x[idx] = temp_val - h\n","    fxh2 = f(x)\n","\n","    grad[idx] = (fxh1 - fxh2) / (2*h)\n","    x[idx] = temp_val\n","\n","  return grad"],"metadata":{"id":"XH1j79P7v80l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(numerical_gradient(function_2,np.array([3.0,4.0])))\n","print(numerical_gradient(function_2,np.array([0.0,2.0])))\n","print(numerical_gradient(function_2,np.array([3.0,0.0])))"],"metadata":{"id":"JZbVvPNn0_Rp","executionInfo":{"status":"ok","timestamp":1642118009673,"user_tz":-540,"elapsed":26,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbc0f573-f923-4e59-ee4b-ed1e4808747e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[6. 8.]\n","[0. 4.]\n","[6. 0.]\n"]}]},{"cell_type":"code","source":["def gradient_descent(f,init_x, lr=0.01, step_num = 100):\n","  x = init_x\n","\n","  for i in range(step_num):\n","    grad = numerical_gradient(f,x)\n","    x -= lr*grad\n","\n","  return x"],"metadata":{"id":"wMiOAjus3oLm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["init_x = np.array([-3.0,4.0])\n","\n","print(gradient_descent(function_2,init_x=init_x,lr=0.1,step_num = 100))"],"metadata":{"id":"SuJpGbzJ5kLm","executionInfo":{"status":"ok","timestamp":1642118009676,"user_tz":-540,"elapsed":26,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cacb6da-a573-46ab-a69f-aaf4b7b6ab1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-6.11110793e-10  8.14814391e-10]\n"]}]},{"cell_type":"code","source":["#import cupy as np\n","\n","def cross_entropy_error(y,t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    if t.size == y.size:\n","      t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    #return -np.sum(t * np.log(y + 1e-7)) / batch_size\n","    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"],"metadata":{"id":"jekmGt_cCP3b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["loss와 y와의 관계는 -log를 따르지만, 가중치 W와의 관계는 더욱 복잡하다."],"metadata":{"id":"nDXXfEoWDO1B"}},{"cell_type":"code","source":["def numerical_gradient(f, x):\n","    h = 1e-4  # 0.0001\n","    grad = np.zeros_like(x)\n","#\n","    it = nump.nditer(x.get(), flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x)  # f(x+h)\n","#\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)  # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","#\n","        x[idx] = tmp_val  # 값 복원\n","        it.iternext()\n","#\n","    return grad\n"],"metadata":{"id":"a5wzJ9C1Kq4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class simpleNet:\n","  def __init__(self):\n","    self.W = np.random.randn(2,3)\n","  \n","  def predict(self,x): # 출력층\n","    return np.dot(x, self.W)\n","\n","  def loss(self, x, t):\n","    z = self.predict(x)\n","    y = softmax(z)\n","    loss = cross_entropy_error(y,t)\n","\n","    return loss"],"metadata":{"id":"AvUXrdKhASFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = simpleNet()\n","print(net.W)\n","\n","x = np.array([0.6,0.9])\n","p = net.predict(x)\n","print(p)\n","print(np.argmax(p))\n","\n","t= np.array([0,0,1])\n","\n","print(net.loss(x,t))\n","\n","def f(blank):\n","  return net.loss(x,t)\n","\n","dW = numerical_gradient(f,net.W)\n","print(dW)\n"],"metadata":{"id":"8GdCEKfYDyUa","executionInfo":{"status":"ok","timestamp":1642118010087,"user_tz":-540,"elapsed":43,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce7ea2be-cc85-431b-b14f-4a187414132d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.58230199  0.39914185 -0.76265673]\n"," [ 1.25606977 -1.48807694 -1.05443914]]\n","[ 2.07984399 -1.09978413 -1.40658926]\n","0\n","3.5561561035079996\n","[[ 0.55959136  0.02327963 -0.58287099]\n"," [ 0.83938704  0.03491944 -0.87430648]]\n"]}]},{"cell_type":"markdown","source":["위 코드 중 numerical_gradient 내부에서 f(x) 값을 구할 때 직접적으로 x 값이 쓰이지 않는다. \n","\n","따라서 x에 어떤 값이 와도 상관없다. 대신 net.W의 값이 바뀌면서 간접적으로 f(x)의 값이 변한다.\n","\n","+ f(x)는 x(입력)의 변화가 아닌(=주어진 이미지는 바뀌지 않음) 가중치의 변화로 인한 생긴 출력값이다."],"metadata":{"id":"dp8pg2XmN-Z4"}},{"cell_type":"code","source":["test = np.array([[[1,2,3],\n","                  [2,1,4],\n","                  [5,2,1],\n","                  [6,3,2]],\n","                 [[5,1,3],\n","                  [1,3,4],\n","                  [4,2,6],\n","                  [3,9,3]],\n","                 [[4,5,6],\n","                  [7,4,3],\n","                  [2,1,5],\n","                  [4,3,1]]])\n","\n","print(np.argmax(test,axis=1))"],"metadata":{"id":"-kgn5DDpO73H","executionInfo":{"status":"ok","timestamp":1642118010088,"user_tz":-540,"elapsed":41,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ccd88aaa-b22a-4c39-ac4c-28586fdcd2a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3 3 1]\n"," [0 3 2]\n"," [1 0 0]]\n"]}]},{"cell_type":"markdown","source":["선형이냐 비선형이냐. => 다양한 모양(기능)을 만들기 위해서 (은닉층을 둔 이유)\n","\n"," 비선형이면 왜 계단함수가 아닌 시그모이드냐 => 미분을 통해 학습시키기 위해서, 가중치를 조금씩 변경시킬때마다 값이 조금씩 변하여 미세조정가능\n"],"metadata":{"id":"c4lbPA4XJb8K"}},{"cell_type":"code","source":["#import cupy as np\n","\n","class TwoLayerNet:\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","    self.params = {}\n","    self.params[\"W1\"] = weight_init_std * np.random.randn(input_size,hidden_size)\n","    self.params[\"b1\"] = np.zeros(hidden_size)\n","    self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n","    self.params[\"b2\"] = np.zeros(output_size)\n","\n","  def predict(self, x):\n","    W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n","    b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n","\n","    a1 = np.dot(x, W1) + b1\n","    z1 = sigmoid(a1)\n","    a2 = np.dot(z1,W2) +b2\n","    y = softmax(a2)\n","\n","    return y\n","\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return cross_entropy_error(y, t)\n","\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis= 1 )\n","    t = np.argmax(t, axis = 1)\n","    accuracy = np.sum(y==t)/float(x.shape[0])\n","    return accuracy\n","\n","  def numerical_gradient(self,x,t):\n","    loss_W = lambda W: self.loss(x,t)\n","\n","    grads={}\n","\n","    grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n","    grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n","    grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n","    grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n","    return grads"],"metadata":{"id":"F_ZgyGL7CeLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#net = TwoLayerNet(input_size=784, hidden_size = 100, output_size = 10)\n","#print(net.params[\"W1\"].shape)\n","#print(net.params[\"b1\"].shape)\n","#print(net.params[\"W2\"].shape)\n","#print(net.params[\"b2\"].shape)\n","#print(net.params[\"b2\"])\n","#print(np.array([1,2,3,4,5,6,7,8,9]).reshape(1,3,3))\n","#x = np.random.rand(100,784)\n","#t = np.random.rand(100,10)\n","#\n","#grads = net.numerical_gradient(x,t)\n","#\n","#print(grads[\"W1\"].shape)\n","#print(grads[\"b1\"].shape)\n","#print(grads[\"W2\"].shape)\n","#print(grads[\"b2\"].shape)"],"metadata":{"id":"y6zyJfgOI_vY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#x = np.random.rand(100,784)\n","#y= net.predict(x)\n","#print(y)"],"metadata":{"id":"0L1aYUUiFYYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","src = list(files.upload().values())[0]\n","open('minist.py','wb').write(src)"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":95},"id":"2cHASFry-WYb","executionInfo":{"status":"ok","timestamp":1642359437319,"user_tz":-540,"elapsed":7689,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"outputId":"f11ae51a-8d33-4619-8551-17c81ec9c45d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-3e41a4b9-91af-4e8c-b0c9-ee06357f0558\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3e41a4b9-91af-4e8c-b0c9-ee06357f0558\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving mnist.py to mnist.py\n"]},{"output_type":"execute_result","data":{"text/plain":["3783"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import cupy as np\n","import numpy as nump\n","\n","from minist import load_mnist\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n","\n","train_loss_list = []\n","train_acc_list=[]\n","test_acc_list = []\n","\n","iter_per_epoch = 1#max(train_size / batch_size, 1)\n","\n","iters_num = 10000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  batch_mask = np.random.choice(train_size,batch_size)\n","  x_batch = np.array(x_train[batch_mask.get()])\n","  t_batch = np.array(t_train[batch_mask.get()])\n","  grad = network.numerical_gradient(x_batch, t_batch)\n","\n","  for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n","    network.params[key] -= learning_rate * grad[key]\n","\n","  loss = network.loss(x_batch,t_batch)\n","  train_loss_list.append(loss)\n","\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(np.array(x_train), np.array(t_train))\n","    test_acc = network.accuracy(np.array(x_test), np.array(t_test))\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test_acc)\n","    print(str(i)+\" train acc, test acc : \"+str(train_acc)+\", \"+str(test_acc))\n","    print(\"loss : \",loss)"],"metadata":{"id":"oUxB_JoR-sJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 합본\n","#import cupy as cp\n","import numpy as np\n","from minist import load_mnist\n","from collections import OrderedDict\n","\n","def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def cross_entropy_error(y,t):\n","    if y.ndim == 1:\n","        y = y.reshape(1, y.size)\n","        t = t.reshape(1, t.size)\n","\n","    if t.size == y.size:\n","      t = t.argmax(axis=1)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4  # 0.0001\n","    grad = np.zeros_like(x)\n","\n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x)  # f(x+h)\n","\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)  # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","\n","        x[idx] = tmp_val  # 값 복원\n","        it.iternext()\n","\n","    return grad\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","class Affine:\n","  def __init__(self, W, b):\n","    self.W = W\n","    self.b = b\n","    self.x = None\n","    self.dW = None\n","    self.db = None\n","\n","  def forward(self, x):\n","    self.x = x\n","    out = np.dot(x, self.W) + self.b\n","\n","    return out\n","  \n","  def backward(self, dout):\n","    dx = np.dot(dout, self.W.T)\n","    self.dW = np.dot(self.x.T, dout)\n","    self.db = np.sum(dout, axis = 0)\n","\n","    return dx\n","\n","class SoftmaxWithLoss:\n","  def __init__(self):\n","    self.loss = None\n","    self.y = None\n","    self.t = None\n","\n","  def forward(self, x, t):\n","    self.t = t\n","    self.y = softmax(x)\n","    self.loss = cross_entropy_error(self.y, self.t)\n","    return self.loss\n","\n","  def backward(self, dout=1):\n","    batch_size = self.t.shape[0]\n","    dx = (self.y-self.t) / batch_size\n","\n","    return dx\n","\n","class Relu:\n","  def __init__(self):\n","    self.mask = None\n","\n","  def forward(self, x):\n","    self.mask = (x <= 0)\n","    out = x.copy()\n","    out[self.mask] = 0\n","    # 포워드에서만 고려하면된다. Why=> 역전파때 다시 쓰이기 때문\n","    # 따라서 카피해서 사용\n","    # 나의 입력(x)이 누군가의 출력(out)이고 해당 값을 저장하고 있을 수 있기 때문이다.\n","    # ex) sigmoid 는 역전파를 위해 out을 저장하고있는데 이를 Relu에 통과시킬때 카피해서 사용 안하면, 해당 값이 변경되기 때문에 역전파에 문제가 생긴다. \n","    # 밑에 \"forward 에서 copy를 하는 이유 !\"에서 설명함\n","\n","    #x[self.mask] = 0\n","    #out = x\n","    return out\n","\n","  def backward(self, dout):\n","    dout[self.mask] = 0\n","    dx = dout\n","\n","    return dx\n","\n","class TwoLayerNet:\n","  def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","    self.params = {}\n","    self.params[\"W1\"] = weight_init_std * np.random.randn(input_size,hidden_size)\n","    self.params[\"b1\"] = np.zeros(hidden_size)\n","    self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n","    self.params[\"b2\"] = np.zeros(output_size)\n","\n","    self.layers = OrderedDict()\n","    self.layers[\"Affine1\"] = Affine(self.params[\"W1\"], self.params[\"b1\"])\n","    self.layers[\"Relu1\"] = Relu()\n","    self.layers[\"Affine2\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n","\n","    self.lastLayer = SoftmaxWithLoss()\n","\n","  def predict(self, x):\n","    #W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n","    #b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n","\n","    #a1 = cp.dot(x, W1) + b1\n","    #z1 = sigmoid(a1)\n","    #a2 = cp.dot(z1,W2) + b2\n","    #y = softmax(a2)\n","    for layer in self.layers.values():\n","      x = layer.forward(x)\n","\n","    return x\n","    #return y\n","\n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    #return cross_entropy_error(y, t)\n","    return self.lastLayer.forward(y, t)\n","\n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis= 1 )\n","    if t.ndim != 1 : t = np.argmax(t, axis = 1)\n","    accuracy = np.sum(y==t)/float(x.shape[0])\n","    return accuracy\n","\n","  def numerical_gradient(self,x,t):\n","    loss_W = lambda W: self.loss(x,t)\n","\n","    grads={}\n","\n","    grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n","    grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n","    grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n","    grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n","    return grads\n","\n","  def gradient(self, x, t):\n","    self.loss(x, t)\n","\n","    dout = 1\n","    dout = self.lastLayer.backward(dout)\n","\n","    layers = list(self.layers.values())\n","    layers.reverse()\n","    for layer in layers:\n","      dout = layer.backward(dout)\n","\n","    grads={}\n","    grads[\"W1\"] = self.layers[\"Affine1\"].dW\n","    grads[\"b1\"] = self.layers[\"Affine1\"].db\n","    grads[\"W2\"] = self.layers[\"Affine2\"].dW\n","    grads[\"b2\"] = self.layers[\"Affine2\"].db\n","\n","    return grads\n","\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size = 50, output_size = 10)\n","x_batch = x_train[:3]\n","t_batch = t_train[:3]\n","\n","grad_backprop = network.gradient(x_batch, t_batch)\n","grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","\n","for key in grad_numerical.keys():\n","  diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","  print(np.abs(grad_backprop[key] - grad_numerical[key]).shape)\n","  print(key + \":\" +str(diff))\n","#train_loss_list = []\n","#train_acc_list=[]\n","#test_acc_list = []\n","#\n","#iter_per_epoch = 1#max(train_size / batch_size, 1)\n","#\n","#iters_num = 10000\n","#train_size = x_train.shape[0]\n","#batch_size = 100\n","#learning_rate = 0.1\n","#\n","#network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","#\n","#for i in range(iters_num):\n","#  batch_mask = np.random.choice(train_size,batch_size)\n","#  x_batch = np.array(x_train[batch_mask])\n","#  t_batch = np.array(t_train[batch_mask])\n","#  grad = network.numerical_gradient(x_batch, t_batch)\n","#\n","#  for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n","#    network.params[key] -= learning_rate * grad[key]\n","#\n","#  loss = network.loss(x_batch,t_batch)\n","#  train_loss_list.append(loss)\n","#\n","#  if i % iter_per_epoch == 0:\n","#    train_acc = network.accuracy(np.array(x_train), np.array(t_train))\n","#    test_acc = network.accuracy(np.array(x_test), np.array(t_test))\n","#    train_acc_list.append(train_acc)\n","#    test_acc_list.append(test_acc)\n","#    print(str(i)+\" train acc, test acc : \"+str(train_acc)+\", \"+str(test_acc))\n","#    print(\"loss : \",loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shCSFQofO7L4","executionInfo":{"status":"ok","timestamp":1642359455418,"user_tz":-540,"elapsed":11831,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"outputId":"82a6fcd3-8922-4de3-c54b-8adf0bd9b4a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading train-images-idx3-ubyte.gz ... \n","Done\n","Downloading train-labels-idx1-ubyte.gz ... \n","Done\n","Downloading t10k-images-idx3-ubyte.gz ... \n","Done\n","Downloading t10k-labels-idx1-ubyte.gz ... \n","Done\n","Converting train-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n","Done\n","Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n","Done\n","Creating pickle file ...\n","Done!\n","(784, 50)\n","W1:2.4749154535990024e-13\n","(50,)\n","b1:7.873227612399669e-13\n","(50, 10)\n","W2:8.673607011116559e-13\n","(10,)\n","b2:1.1990408943507446e-10\n"]}]},{"cell_type":"markdown","source":["오차역전파 : 덧셈은 상류를 바로 하류로 보낸다. 곱셈은 상류의 값에다가 나를 제외한 다른 값들을 곱한다.(ex 2개일 경우 스위칭된다.)\n","\n","p86의 그래프에서 노드와 노드 사이의 계산이 하나의 \"계층\"이다.\n","\n","\"계층\"에서의 노드는 연산, 에지는 값이 흐른다.(ps p86그래프에서 노드는 값, 에지는 가중치 값)\n","\n","ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n","\n","ReLu 장점 : 0보다 작은 부분들은 활성화가 안됨 => 특정 뉴런들만이 동작함 \n","\n","왜 사용하는가? => 무슨 일(생각)을 할 때 \"모든\" 뉴련이 동작할 필요는 없음"],"metadata":{"id":"IHElmhEaeKBV"}},{"cell_type":"code","source":["class Relu:\n","  def __init__(self):\n","    self.mask = None\n","  def forward(self,x):\n","    self.mask = (x<=0)\n","\n","    x[self.mask] = 0\n","    out = x\n","\n","    return out\n","\n","  def backward(self,dout):\n","    dout[self.mask] = 0\n","    dx = dout\n","\n","    return dx;\n","\n","  #p167"],"metadata":{"id":"R0lPAone19Kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward 에서 copy를 하는 이유 !\n","import numpy as np\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = sigmoid(x)\n","        out[0] = -77\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","class Relu2:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n"," \n","        x[self.mask] = 0\n","        out =x\n","        return out\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","x=np.random.randn(5)\n","print(x)\n","print(\"===========================================\")\n","\n","sig_1 = Sigmoid()\n","sig_2 = Sigmoid()\n","sig_3 = Sigmoid()\n","sig_4 = Sigmoid()\n","\n","\n","re_1 = Relu()\n","re_2 = Relu()\n","\n","re2_1 = Relu2()\n","re2_2 = Relu2()\n","\n","\n","result1_1 = sig_1.forward(x)\n","print(sig_1.out)\n","print(result1_1)\n","result1_2 = re_1.forward(result1_1)\n","print(result1_2)\n","result1_3 = re_1.backward(result1_2)\n","print(result1_3)\n","result1_4 = sig_1.backward(result1_3)\n","print(result1_4)\n","print(sig_1.out)\n","print(\"===========================================\")\n","result2_1 = sig_2.forward(x)\n","print(sig_2.out)\n","print(result2_1)\n","result2_2 = re2_1.forward(result2_1)\n","print(result2_2)\n","result2_3 = re2_1.backward(result2_2)\n","print(result2_3)\n","result2_4 = sig_2.backward(result2_3)\n","print(result2_4)\n","print(sig_2.out)\n","print(\"===========================================\")\n","#print(x)\n","print(sig_1.out)\n","print(sig_1.backward(x))\n","#print(x)\n","print(sig_1.out - sig_2.out)\n","print(sig_2.out)\n","print(sig_2.backward(x))\n","\n","\n"],"metadata":{"id":"70t9we3Knt0w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642360062022,"user_tz":-540,"elapsed":261,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"outputId":"9c84eabd-2b11-4f21-fa8c-370722a64406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.40114532 -0.06897942 -0.74440744  0.9260605   0.52573732]\n","===========================================\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","[-0.          0.12054704  0.07031144  0.14556504  0.14674621]\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","===========================================\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","[0.         0.12054704 0.07031144 0.14556504 0.14674621]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","===========================================\n","[-77.           0.48276198   0.32204111   0.71627536   0.62848836]\n","[ 2.40927880e+03 -1.72243576e-02 -1.62526947e-01  1.88198614e-01\n","  1.22754797e-01]\n","[-77.   0.   0.   0.   0.]\n","[0.         0.48276198 0.32204111 0.71627536 0.62848836]\n","[-0.         -0.01722436 -0.16252695  0.18819861  0.1227548 ]\n"]}]},{"cell_type":"code","source":["(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n","\n","network = TwoLayerNet(input_size=784, hidden_size = 50, output_size = 10)\n","\n","train_loss_list = []\n","train_acc_list=[]\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","iters_num = 30000\n","train_size = x_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","for i in range(iters_num):\n","  batch_mask = np.random.choice(train_size,batch_size)\n","  x_batch = np.array(x_train[batch_mask])\n","  t_batch = np.array(t_train[batch_mask])\n","  grad = network.gradient(x_batch, t_batch)\n","\n","  for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n","    network.params[key] -= learning_rate * grad[key]\n","\n","  loss = network.loss(x_batch,t_batch)\n","  train_loss_list.append(loss)\n","\n","  if i % iter_per_epoch == 0:\n","    train_acc = network.accuracy(np.array(x_train), np.array(t_train))\n","    test_acc = network.accuracy(np.array(x_test), np.array(t_test))\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test_acc)\n","    print(str(i)+\" train acc, test acc : \"+str(train_acc)+\", \"+str(test_acc))\n","    print(\"loss : \",loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFKxfxiNSoZP","executionInfo":{"status":"ok","timestamp":1642360585258,"user_tz":-540,"elapsed":146678,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"outputId":"b4920db3-0a2a-48a1-b88d-29899135aecb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 train acc, test acc : 0.13928333333333334, 0.144\n","loss :  2.3008955585807347\n","600 train acc, test acc : 0.90285, 0.9074\n","loss :  0.26312452226471594\n","1200 train acc, test acc : 0.9220666666666667, 0.9251\n","loss :  0.17343815007035446\n","1800 train acc, test acc : 0.9356333333333333, 0.9329\n","loss :  0.24708421427938063\n","2400 train acc, test acc : 0.9449666666666666, 0.9428\n","loss :  0.24003391406504676\n","3000 train acc, test acc : 0.9529666666666666, 0.9509\n","loss :  0.11287139860197275\n","3600 train acc, test acc : 0.9560166666666666, 0.9524\n","loss :  0.15652048099236385\n","4200 train acc, test acc : 0.9633, 0.9598\n","loss :  0.08029930160427781\n","4800 train acc, test acc : 0.96645, 0.9613\n","loss :  0.11822997397710162\n","5400 train acc, test acc : 0.9676833333333333, 0.9625\n","loss :  0.08835182732365561\n","6000 train acc, test acc : 0.9719166666666667, 0.9663\n","loss :  0.057117778321073905\n","6600 train acc, test acc : 0.9733166666666667, 0.9659\n","loss :  0.07893803972628005\n","7200 train acc, test acc : 0.9748666666666667, 0.9686\n","loss :  0.029597394583965927\n","7800 train acc, test acc : 0.9753, 0.9672\n","loss :  0.05122386304747593\n","8400 train acc, test acc : 0.9772166666666666, 0.969\n","loss :  0.09773460219373784\n","9000 train acc, test acc : 0.97755, 0.9699\n","loss :  0.06215567125133041\n","9600 train acc, test acc : 0.97975, 0.97\n","loss :  0.03039267256425067\n","10200 train acc, test acc : 0.9812333333333333, 0.9722\n","loss :  0.04252043906684059\n","10800 train acc, test acc : 0.9820166666666666, 0.9721\n","loss :  0.02967507772194246\n","11400 train acc, test acc : 0.9814333333333334, 0.9712\n","loss :  0.08640462017867191\n","12000 train acc, test acc : 0.9831833333333333, 0.9726\n","loss :  0.04768545978574182\n","12600 train acc, test acc : 0.9837666666666667, 0.9733\n","loss :  0.019307016665455604\n","13200 train acc, test acc : 0.9843333333333333, 0.9735\n","loss :  0.022216401552550426\n","13800 train acc, test acc : 0.9853166666666666, 0.974\n","loss :  0.014420991760030184\n","14400 train acc, test acc : 0.9852, 0.9739\n","loss :  0.026433279577408587\n","15000 train acc, test acc : 0.986, 0.9726\n","loss :  0.009048948339035363\n","15600 train acc, test acc : 0.9868333333333333, 0.9756\n","loss :  0.028471225673906333\n","16200 train acc, test acc : 0.9869333333333333, 0.9741\n","loss :  0.030859714877601446\n","16800 train acc, test acc : 0.9872, 0.9732\n","loss :  0.03744765861950914\n","17400 train acc, test acc : 0.9872, 0.9745\n","loss :  0.010946253857510246\n","18000 train acc, test acc : 0.9889666666666667, 0.9741\n","loss :  0.034022887087118876\n","18600 train acc, test acc : 0.99005, 0.974\n","loss :  0.011468728068215838\n","19200 train acc, test acc : 0.9905833333333334, 0.9742\n","loss :  0.012474875520733958\n","19800 train acc, test acc : 0.99065, 0.9731\n","loss :  0.008415069498038769\n","20400 train acc, test acc : 0.9910666666666667, 0.9747\n","loss :  0.05007092844493807\n","21000 train acc, test acc : 0.99165, 0.9751\n","loss :  0.017060194855323656\n","21600 train acc, test acc : 0.9910666666666667, 0.9747\n","loss :  0.015493255219494927\n","22200 train acc, test acc : 0.99195, 0.9744\n","loss :  0.03362494489818433\n","22800 train acc, test acc : 0.9924833333333334, 0.9747\n","loss :  0.011873587415741212\n","23400 train acc, test acc : 0.9921, 0.9741\n","loss :  0.010613398097634903\n","24000 train acc, test acc : 0.9932, 0.9733\n","loss :  0.007013252600276909\n","24600 train acc, test acc : 0.9932833333333333, 0.9745\n","loss :  0.009538812539691762\n","25200 train acc, test acc : 0.9938666666666667, 0.9743\n","loss :  0.012939540538370216\n","25800 train acc, test acc : 0.9937, 0.9744\n","loss :  0.005643850672628149\n","26400 train acc, test acc : 0.9941, 0.9736\n","loss :  0.023437114658541484\n","27000 train acc, test acc : 0.9946666666666667, 0.9748\n","loss :  0.035720557405281246\n","27600 train acc, test acc : 0.9947833333333334, 0.9748\n","loss :  0.006680038131730171\n","28200 train acc, test acc : 0.9945, 0.9736\n","loss :  0.017859424995139105\n","28800 train acc, test acc : 0.99445, 0.974\n","loss :  0.007227184096607358\n","29400 train acc, test acc : 0.9949333333333333, 0.9747\n","loss :  0.012310564065366413\n"]}]},{"cell_type":"code","source":["np.random.randn(2,10) *1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOd9nzTJa38Y","executionInfo":{"status":"ok","timestamp":1642362357796,"user_tz":-540,"elapsed":257,"user":{"displayName":"GH K","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01062405385549991714"}},"outputId":"43595b66-f50c-4671-ec79-cd67f651b060"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.56569847,  0.85512665,  0.78766226, -0.80050378, -0.14906627,\n","        -1.30568827,  0.05759241, -1.86100452, -0.98789379,  1.983349  ],\n","       [-0.59076857,  0.57793377, -0.94554218,  1.45748826, -1.33054863,\n","        -1.13570648,  1.1455276 , -0.34992864, -1.59036835,  0.71020415]])"]},"metadata":{},"execution_count":29}]}]}